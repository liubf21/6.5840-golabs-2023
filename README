
## Lab1

https://github.com/OneSizeFitsQuorum/MIT6.824-2021
比我想象的难一些，csdiy上提供的链接只讲了实现难点，看不太懂，因为我还在进行基础思路的部分，主要是仔细看官网上的讲义、Lab介绍和大佬写的课程中文翻译，我的英文阅读能力还是有待增强，论文不太读得下去

实现 coordinator  worker rpc

our job is to implement a distributed MapReduce, consisting of two programs, the coordinator and the worker. There will be just one coordinator process, and one or more worker processes executing in parallel. In a real system the workers would run on a bunch of different machines, but for this lab you'll run them all on a single machine. 
The workers will talk to the coordinator via RPC. Each worker process will ask the coordinator for a task, read the task's input from one or more files, execute the task, and write the task's output to one or more files. The coordinator should notice if a worker hasn't completed its task in a reasonable amount of time (for this lab, use ten seconds), and give the same task to a different worker.

worker 通过 RPC 获取 task
Map 得到很多 k-v 对(其中有大量重复的k)  emit 传入k为单词，v为'1'
Reduce 由 k-v 对 得到每一个k 所对应的k-v对的个数  emit 传入v为每个k对应的个数(数组的长度)

很多 Map 可以并行，每个读取一个文件，得到一个 intermediate data  mr-X-Y 
全部运行完之后则运行Reduce，Reduce应该读对应哈希桶中的数据(从所有mr-*-Y文件中读取键值对)，输出到对应桶中
一开始的想法是使用 filepath.Walk 遍历全部文件再判断后缀是否匹配，不够优雅
用锁来实现并发

coordinator 
分配任务 首先分配Map任务，并等待全部Map任务执行完成，之后分配Reduce任务，并等待Reduce任务执行完成
最开始得到文件名 files，收到分配任务的请求时，分配一个Map任务，
需要检测是否完成任务，逐个检查麻烦，让worker在完成任务时发送消息
x 如果该任务10s没完成则会被分配给其他worker

RPC 构成: 任务类型 索引(任务编号) 
Map的文件名 Map任务数
Reduce任务数

读写的原子性
崩溃

strconv.Itoa 转换整数为字符串

没加锁 但单线程为什么错 循环中没有对变量重新初始化，而在传递时，有些没用到的变量又没有更改

简陋的实现了一版，测试中除了crash test，别的都过了，但还没实现并行和加锁

```
go build -buildmode=plugin ../mrapps/wc.go
rm mr-*

go run mrcoordinator.go pg-*.txt

go run mrworker.go wc.so
```



MapReduce hides many details:
  sending app code to servers
  tracking which tasks have finished
  "shuffling" intermediate data from Maps to Reduces
  balancing load over servers
  recovering from failures

  The "Coordinator" manages all the steps in a job.
  1. coordinator gives Map tasks to workers until all Maps complete
     Maps write output (intermediate data) to local disk
     Maps split output, by hash(key) mod R, into one file per Reduce task
  2. after all Maps have finished, coordinator hands out Reduce tasks
     each Reduce task corresponds to one hash bucket of intermediate output
     each Reduce fetches its intermediate output from (all) Map workers
     each Reduce task writes a separate output file on GFS

Go RPC sends only struct fields whose names start with capital letters. Sub-structures must also have capitalized field names.
